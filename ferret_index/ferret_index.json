{
  "src\\analysis\\mod.rs": "pub mod duplicates;\r\n",
  "src\\search\\engine.rs": "use std::path::{Path, PathBuf};\r\nuse anyhow::Result;\r\n// use tantivy::{\r\n//     collector::TopDocs,\r\n//     directory::MmapDirectory,\r\n//     query::QueryParser,\r\n//     schema::{Schema, Field, STORED, TEXT},\r\n//     Index, IndexReader, IndexWriter, ReloadPolicy, Term,\r\n// };\r\nuse ignore::WalkBuilder;\r\nuse crate::extraction::document::DocumentExtractor;\r\n\r\npub struct RipgrepSearchEngine {\r\n    index_path: std::path::PathBuf,\r\n}\r\n\r\npub struct RipgrepIntegration;\r\n\r\nimpl RipgrepIntegration {\r\n    pub async fn search_with_ripgrep(&self, pattern: &str, path: &Path) -> Result<Vec<SearchResult>> {\r\n        let extractor = DocumentExtractor::new();\r\n        let mut file_results: std::collections::HashMap<PathBuf, Vec<(u64, String)>> = std::collections::HashMap::new();\r\n        \r\n        // First pass: collect all matches per file\r\n        for entry in WalkBuilder::new(path).build() {\r\n            let entry = entry?;\r\n            if !entry.file_type().map_or(false, |ft| ft.is_file()) {\r\n                continue;\r\n            }\r\n            \r\n            let file_path = entry.path();\r\n            let file_extension = file_path.extension()\r\n                .and_then(|s| s.to_str())\r\n                .unwrap_or(\"\")\r\n                .to_lowercase();\r\n            \r\n            // For document files, extract content first\r\n            if matches!(file_extension.as_str(), \"pdf\" | \"docx\" | \"xlsx\" | \"pptx\") {\r\n                match extractor.extract_content(file_path).await {\r\n                    Ok(content) => {\r\n                        // Search in extracted content\r\n                        let lines: Vec<&str> = content.lines().collect();\r\n                        for (line_num, line) in lines.iter().enumerate() {\r\n                            if line.to_lowercase().contains(&pattern.to_lowercase()) {\r\n                                file_results.entry(file_path.to_path_buf())\r\n                                    .or_insert_with(Vec::new)\r\n                                    .push((line_num as u64 + 1, line.to_string()));\r\n                            }\r\n                        }\r\n                    }\r\n                    Err(e) => {\r\n                        log::warn!(\"Failed to extract content from {}: {}\", file_path.display(), e);\r\n                        continue;\r\n                    }\r\n                }\r\n            } else {\r\n                // For text files, search directly\r\n                if let Ok(content) = std::fs::read_to_string(file_path) {\r\n                    let lines: Vec<&str> = content.lines().collect();\r\n                    for (line_num, line) in lines.iter().enumerate() {\r\n                        if line.to_lowercase().contains(&pattern.to_lowercase()) {\r\n                            file_results.entry(file_path.to_path_buf())\r\n                                .or_insert_with(Vec::new)\r\n                                .push((line_num as u64 + 1, line.to_string()));\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        \r\n        // Second pass: create rich search results with context\r\n        let mut results = Vec::new();\r\n        for (file_path, matches) in file_results {\r\n            let file_size = std::fs::metadata(&file_path).map(|m| m.len()).unwrap_or(0);\r\n            let file_type = file_path.extension()\r\n                .and_then(|s| s.to_str())\r\n                .unwrap_or(\"unknown\")\r\n                .to_string();\r\n            let match_count = matches.len();\r\n            \r\n            // Get file content for context\r\n            let file_content = if matches!(file_type.as_str(), \"pdf\" | \"docx\" | \"xlsx\" | \"pptx\") {\r\n                extractor.extract_content(&file_path).await.unwrap_or_default()\r\n            } else {\r\n                std::fs::read_to_string(&file_path).unwrap_or_default()\r\n            };\r\n            \r\n            let lines: Vec<&str> = file_content.lines().collect();\r\n            \r\n            // Group nearby matches to avoid duplicate results\r\n            let mut processed_lines = std::collections::HashSet::new();\r\n            \r\n            for (line_num, line_content) in &matches {\r\n                if processed_lines.contains(line_num) {\r\n                    continue;\r\n                }\r\n                \r\n                // Calculate context (3 lines before and after)\r\n                let context_before: Vec<String> = lines\r\n                    .iter()\r\n                    .skip((line_num.saturating_sub(4)) as usize)\r\n                    .take(3)\r\n                    .map(|s| s.to_string())\r\n                    .collect();\r\n                \r\n                let context_after: Vec<String> = lines\r\n                    .iter()\r\n                    .skip((line_num + 1) as usize)\r\n                    .take(3)\r\n                    .map(|s| s.to_string())\r\n                    .collect();\r\n                \r\n                // Calculate relevance score\r\n                let score = self.calculate_relevance_score(line_content, pattern, &file_path);\r\n                \r\n                // Mark nearby lines as processed to avoid duplicates\r\n                for i in line_num.saturating_sub(2)..=line_num + 2 {\r\n                    processed_lines.insert(i);\r\n                }\r\n                \r\n                results.push(SearchResult {\r\n                    path: file_path.clone(),\r\n                    score,\r\n                    snippet: line_content.clone(),\r\n                    line_number: Some(*line_num),\r\n                    content: Some(line_content.clone()),\r\n                    context_before,\r\n                    context_after,\r\n                    match_count,\r\n                    file_size,\r\n                    file_type: file_type.clone(),\r\n                });\r\n            }\r\n        }\r\n        \r\n        // Sort by relevance score (highest first)\r\n        results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap_or(std::cmp::Ordering::Equal));\r\n        \r\n        Ok(results)\r\n    }\r\n    \r\n    fn calculate_relevance_score(&self, line: &str, pattern: &str, file_path: &Path) -> f32 {\r\n        let mut score = 1.0;\r\n        \r\n        // Boost score for exact matches\r\n        if line.to_lowercase().contains(&pattern.to_lowercase()) {\r\n            score += 2.0;\r\n        }\r\n        \r\n        // Boost score for multiple occurrences in the line\r\n        let occurrences = line.to_lowercase().matches(&pattern.to_lowercase()).count();\r\n        score += occurrences as f32 * 0.5;\r\n        \r\n        // Boost score for filename matches\r\n        if let Some(filename) = file_path.file_name().and_then(|n| n.to_str()) {\r\n            if filename.to_lowercase().contains(&pattern.to_lowercase()) {\r\n                score += 1.5;\r\n            }\r\n        }\r\n        \r\n        // Boost score for shorter lines (more precise matches)\r\n        if line.len() < 100 {\r\n            score += 0.5;\r\n        }\r\n        \r\n        score\r\n    }\r\n}\r\n\r\n\r\nimpl RipgrepSearchEngine {\r\n    pub fn new(index_path: &Path) -> Result<Self> {\r\n        Ok(Self {\r\n            index_path: index_path.to_path_buf(),\r\n        })\r\n    }\r\n    \r\n    pub async fn index_file(&mut self, path: &Path, _metadata: &FileMetadata) -> Result<()> {\r\n        use std::collections::HashMap;\r\n        use serde_json;\r\n        \r\n        // Create index directory if it doesn't exist\r\n        std::fs::create_dir_all(&self.index_path)?;\r\n        \r\n        // Extract content using DocumentExtractor\r\n        let extractor = DocumentExtractor::new();\r\n        let content = match extractor.extract_content(path).await {\r\n            Ok(content) => content,\r\n            Err(_) => return Ok(()), // Skip files that can't be processed\r\n        };\r\n        \r\n        // Store in a simple JSON index file\r\n        let index_file = self.index_path.join(\"ferret_index.json\");\r\n        let mut index: HashMap<String, String> = if index_file.exists() {\r\n            let data = tokio::fs::read_to_string(&index_file).await?;\r\n            serde_json::from_str(&data).unwrap_or_default()\r\n        } else {\r\n            HashMap::new()\r\n        };\r\n        \r\n        // Add file to index\r\n        index.insert(path.to_string_lossy().to_string(), content);\r\n        \r\n        // Write back to index file\r\n        let json = serde_json::to_string_pretty(&index)?;\r\n        tokio::fs::write(&index_file, json).await?;\r\n        \r\n        Ok(())\r\n    }\r\n    \r\n    pub async fn search(&self, query_str: &str, limit: usize) -> Result<Vec<SearchResult>> {\r\n        use std::collections::HashMap;\r\n        use serde_json;\r\n        \r\n        let index_file = self.index_path.join(\"ferret_index.json\");\r\n        if !index_file.exists() {\r\n            return Ok(Vec::new());\r\n        }\r\n        \r\n        // Load index\r\n        let data = tokio::fs::read_to_string(&index_file).await?;\r\n        let index: HashMap<String, String> = serde_json::from_str(&data)?;\r\n        \r\n        let mut results = Vec::new();\r\n        let query_lower = query_str.to_lowercase();\r\n        \r\n        for (file_path_str, content) in index {\r\n            let file_path = std::path::PathBuf::from(&file_path_str);\r\n            let lines: Vec<&str> = content.lines().collect();\r\n            let mut matches = Vec::new();\r\n            \r\n            // Search in content\r\n            for (line_num, line) in lines.iter().enumerate() {\r\n                if line.to_lowercase().contains(&query_lower) {\r\n                    matches.push((line_num as u64 + 1, line.to_string()));\r\n                }\r\n            }\r\n            \r\n            if !matches.is_empty() {\r\n                let file_size = std::fs::metadata(&file_path).map(|m| m.len()).unwrap_or(0);\r\n                let file_type = file_path.extension()\r\n                    .and_then(|s| s.to_str())\r\n                    .unwrap_or(\"unknown\")\r\n                    .to_string();\r\n                let match_count = matches.len();\r\n                \r\n                // Create search results for each match\r\n                for (line_num, line_content) in matches {\r\n                    let context_before: Vec<String> = lines\r\n                        .iter()\r\n                        .skip((line_num.saturating_sub(4)) as usize)\r\n                        .take(3)\r\n                        .map(|s| s.to_string())\r\n                        .collect();\r\n                    \r\n                    let context_after: Vec<String> = lines\r\n                        .iter()\r\n                        .skip((line_num + 1) as usize)\r\n                        .take(3)\r\n                        .map(|s| s.to_string())\r\n                        .collect();\r\n                    \r\n                    let score = self.calculate_relevance_score(&line_content, query_str, &file_path);\r\n                    \r\n                    results.push(SearchResult {\r\n                        path: file_path.clone(),\r\n                        score,\r\n                        snippet: line_content.clone(),\r\n                        line_number: Some(line_num),\r\n                        content: Some(line_content),\r\n                        context_before,\r\n                        context_after,\r\n                        match_count,\r\n                        file_size,\r\n                        file_type: file_type.clone(),\r\n                    });\r\n                }\r\n            }\r\n        }\r\n        \r\n        // Sort by relevance score and apply limit\r\n        results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap_or(std::cmp::Ordering::Equal));\r\n        results.truncate(limit);\r\n        \r\n        Ok(results)\r\n    }\r\n    \r\n    fn calculate_relevance_score(&self, line: &str, pattern: &str, file_path: &std::path::Path) -> f32 {\r\n        let mut score = 1.0;\r\n        \r\n        // Boost score for exact matches\r\n        if line.to_lowercase().contains(&pattern.to_lowercase()) {\r\n            score += 2.0;\r\n        }\r\n        \r\n        // Boost score for multiple occurrences in the line\r\n        let occurrences = line.to_lowercase().matches(&pattern.to_lowercase()).count();\r\n        score += occurrences as f32 * 0.5;\r\n        \r\n        // Boost score for filename matches\r\n        if let Some(filename) = file_path.file_name().and_then(|n| n.to_str()) {\r\n            if filename.to_lowercase().contains(&pattern.to_lowercase()) {\r\n                score += 1.5;\r\n            }\r\n        }\r\n        \r\n        // Boost score for shorter lines (more precise matches)\r\n        if line.len() < 100 {\r\n            score += 0.5;\r\n        }\r\n        \r\n        score\r\n    }\r\n    \r\n    pub fn commit(&mut self) -> Result<()> {\r\n        // Index is automatically committed on each file addition\r\n        Ok(())\r\n    }\r\n}\r\n\r\n#[derive(Debug, Clone)]\r\npub struct SearchResult {\r\n    pub path: PathBuf,\r\n    pub score: f32,\r\n    pub snippet: String,\r\n    pub line_number: Option<u64>,\r\n    pub content: Option<String>,\r\n    pub context_before: Vec<String>,\r\n    pub context_after: Vec<String>,\r\n    pub match_count: usize,\r\n    pub file_size: u64,\r\n    pub file_type: String,\r\n}\r\n\r\n#[derive(Debug)]\r\npub struct FileMetadata {\r\n    pub size: u64,\r\n    pub modified: std::time::SystemTime,\r\n    pub is_binary: bool,\r\n}\r\n",
  "src\\analysis\\duplicates.rs": "use std::collections::HashMap;\r\nuse std::path::{Path, PathBuf};\r\nuse sha2::{Sha256, Digest};\r\nuse anyhow::Result;\r\nuse tokio::io::AsyncReadExt;\r\nuse crate::file_discovery::FileGroup;\r\n\r\npub struct SmartDuplicateDetector;\r\n\r\nimpl SmartDuplicateDetector {\r\n    pub fn new() -> Self {\r\n        Self\r\n    }\r\n    \r\n    pub async fn detect_duplicates(&self, file_groups: &[FileGroup]) -> Result<DuplicateResults> {\r\n        let mut results = DuplicateResults::new();\r\n        \r\n        for group in file_groups {\r\n            if group.is_potential_duplicate() {\r\n                // Find actual duplicates within this group\r\n                let duplicate_sets = self.find_exact_duplicates_in_group(group).await?;\r\n                \r\n                if !duplicate_sets.is_empty() {\r\n                    results.add_duplicate_group(DuplicateGroup {\r\n                        base_name: group.canonical_name.clone(),\r\n                        duplicate_sets,\r\n                    });\r\n                }\r\n            }\r\n        }\r\n        \r\n        Ok(results)\r\n    }\r\n    \r\n    async fn find_exact_duplicates_in_group(&self, group: &FileGroup) -> Result<Vec<Vec<PathBuf>>> {\r\n        // Hash all files in the group\r\n        let mut hash_map: HashMap<String, Vec<PathBuf>> = HashMap::new();\r\n        \r\n        for file_path in &group.variants {\r\n            // Skip if file doesn't exist or can't be read\r\n            if !file_path.exists() {\r\n                log::warn!(\"File doesn't exist: {:?}\", file_path);\r\n                continue;\r\n            }\r\n            \r\n            match self.hash_file(file_path).await {\r\n                Ok(hash) => {\r\n                    hash_map.entry(hash)\r\n                        .or_insert_with(Vec::new)\r\n                        .push(file_path.clone());\r\n                }\r\n                Err(e) => {\r\n                    log::warn!(\"Failed to hash file {:?}: {}\", file_path, e);\r\n                }\r\n            }\r\n        }\r\n        \r\n        // Return only groups with 2+ files (actual duplicates)\r\n        Ok(hash_map\r\n            .into_values()\r\n            .filter(|group| group.len() > 1)\r\n            .collect())\r\n    }\r\n    \r\n    async fn hash_file(&self, path: &Path) -> Result<String> {\r\n        let mut file = tokio::fs::File::open(path).await?;\r\n        let mut hasher = Sha256::new();\r\n        let mut buffer = vec![0u8; 8192];\r\n        \r\n        loop {\r\n            let bytes_read = file.read(&mut buffer).await?;\r\n            if bytes_read == 0 {\r\n                break;\r\n            }\r\n            hasher.update(&buffer[..bytes_read]);\r\n        }\r\n        \r\n        Ok(format!(\"{:x}\", hasher.finalize()))\r\n    }\r\n}\r\n\r\n#[derive(Debug, serde::Serialize)]\r\npub struct DuplicateResults {\r\n    pub total_duplicates: usize,\r\n    pub space_wasted: u64,\r\n    pub duplicate_groups: Vec<DuplicateGroup>,\r\n}\r\n\r\nimpl DuplicateResults {\r\n    pub fn new() -> Self {\r\n        Self {\r\n            total_duplicates: 0,\r\n            space_wasted: 0,\r\n            duplicate_groups: Vec::new(),\r\n        }\r\n    }\r\n    \r\n    pub fn add_duplicate_group(&mut self, group: DuplicateGroup) {\r\n        // Calculate stats\r\n        for duplicate_set in &group.duplicate_sets {\r\n            if duplicate_set.len() > 1 {\r\n                // Count actual duplicate files (all but one original)\r\n                self.total_duplicates += duplicate_set.len() - 1;\r\n                \r\n                // Calculate wasted space\r\n                for file in duplicate_set.iter().skip(1) {\r\n                    if let Ok(metadata) = std::fs::metadata(file) {\r\n                        self.space_wasted += metadata.len();\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        \r\n        self.duplicate_groups.push(group);\r\n    }\r\n}\r\n\r\n#[derive(Debug, serde::Serialize)]\r\npub struct DuplicateGroup {\r\n    pub base_name: String,\r\n    pub duplicate_sets: Vec<Vec<PathBuf>>,  // Each inner vec contains identical files\r\n}",
  "src\\main.rs": "use clap::{Parser, Subcommand};\r\nuse std::path::PathBuf;\r\nuse anyhow::Result;\r\n\r\nmod file_discovery;\r\nmod io;\r\nmod analysis;\r\nmod search;\r\nmod extraction;\r\nmod integrations;\r\nmod platform;\r\n\r\nuse platform::FerretPlatform;\r\n\r\n#[derive(Parser)]\r\n#[command(name = \"ferret\")]\r\n#[command(about = \"A powerful file analysis and search tool for cleaning up messy shared drives\")]\r\nstruct Cli {\r\n    #[command(subcommand)]\r\n    command: Commands,\r\n}\r\n\r\n#[derive(Subcommand)]\r\nenum Commands {\r\n    /// Analyze directory for duplicates and similar files\r\n    Analyze {\r\n        /// Directory to analyze\r\n        path: PathBuf,\r\n        /// Output format (json, text)\r\n        #[arg(short, long, default_value = \"text\")]\r\n        format: String,\r\n    },\r\n    /// Search for files and content\r\n    Search {\r\n        /// Search query\r\n        query: String,\r\n        /// Directory to search in\r\n        path: PathBuf,\r\n        /// Maximum number of results\r\n        #[arg(short, long, default_value = \"100\")]\r\n        limit: usize,\r\n    },\r\n    /// Index directory for fast searching\r\n    Index {\r\n        /// Directory to index\r\n        path: PathBuf,\r\n        /// Index location\r\n        #[arg(short, long, default_value = \"./ferret_index\")]\r\n        index_path: PathBuf,\r\n    },\r\n}\r\n\r\n#[tokio::main]\r\nasync fn main() -> Result<()> {\r\n    env_logger::init();\r\n    \r\n    let cli = Cli::parse();\r\n    \r\n    match cli.command {\r\n        Commands::Analyze { path, format } => {\r\n            if !path.exists() {\r\n                eprintln!(\"Error: Path does not exist: {}\", path.display());\r\n                std::process::exit(1);\r\n            }\r\n            \r\n            let mut platform = FerretPlatform::new()?;\r\n            let results = platform.analyze_directory(&path).await?;\r\n            \r\n            match format.as_str() {\r\n                \"json\" => println!(\"{}\", serde_json::to_string_pretty(&results)?),\r\n                _ => println!(\"{:#?}\", results),\r\n            }\r\n        }\r\n        Commands::Search { query, path, limit } => {\r\n            if query.is_empty() {\r\n                eprintln!(\"Error: Search query cannot be empty\");\r\n                std::process::exit(1);\r\n            }\r\n            \r\n            if limit == 0 {\r\n                eprintln!(\"Error: Limit must be greater than 0\");\r\n                std::process::exit(1);\r\n            }\r\n            \r\n            if !path.exists() {\r\n                eprintln!(\"Error: Path does not exist: {}\", path.display());\r\n                std::process::exit(1);\r\n            }\r\n            \r\n            let mut platform = FerretPlatform::new()?;\r\n            \r\n            // Check if there's an index in the current directory\r\n            let index_path = std::path::Path::new(\"./ferret_index\");\r\n            if index_path.exists() {\r\n                // Load the existing index\r\n                let engine = crate::search::engine::RipgrepSearchEngine::new(index_path)?;\r\n                platform.set_search_engine(engine);\r\n            }\r\n            \r\n            let results = platform.search(&query, &path, limit).await?;\r\n            \r\n            for (i, result) in results.iter().enumerate() {\r\n                if i > 0 {\r\n                    println!(); // Add spacing between results\r\n                }\r\n                \r\n                // Header with file info\r\n                println!(\"🔍 {} (Score: {:.1}, {} matches, {} bytes, .{})\", \r\n                    result.path.display(), \r\n                    result.score,\r\n                    result.match_count,\r\n                    result.file_size,\r\n                    result.file_type\r\n                );\r\n                \r\n                // Context before\r\n                for (j, context_line) in result.context_before.iter().enumerate() {\r\n                    let base_line = result.line_number.unwrap_or(0);\r\n                    let line_num = base_line.saturating_sub((result.context_before.len() - j) as u64);\r\n                    println!(\"  {:3} │ {}\", line_num, context_line);\r\n                }\r\n                \r\n                // Main match line (highlighted)\r\n                if let Some(line_num) = result.line_number {\r\n                    println!(\"  {:3} │ {} ← MATCH\", line_num, result.snippet);\r\n                }\r\n                \r\n                // Context after\r\n                for (j, context_line) in result.context_after.iter().enumerate() {\r\n                    let line_num = result.line_number.unwrap_or(0) + (j + 1) as u64;\r\n                    println!(\"  {:3} │ {}\", line_num, context_line);\r\n                }\r\n            }\r\n        }\r\n        Commands::Index { path, index_path } => {\r\n            if !path.exists() {\r\n                eprintln!(\"Error: Path does not exist: {}\", path.display());\r\n                std::process::exit(1);\r\n            }\r\n            \r\n            let mut platform = FerretPlatform::new()?;\r\n            platform.index_directory(&path, &index_path).await?;\r\n            println!(\"Indexing complete for: {}\", path.display());\r\n        }\r\n    }\r\n    \r\n    Ok(())\r\n}\r\n",
  "src\\lib.rs": "pub mod analysis;\r\npub mod file_discovery;\r\npub mod platform;\r\npub mod search;\r\npub mod io;\r\npub mod extraction;\r\npub mod integrations;",
  "src\\io\\mod.rs": "",
  "src\\integrations\\mod.rs": "",
  "src\\extraction\\document.rs": "use std::path::Path;\r\nuse anyhow::Result;\r\nuse std::process::Command;\r\n\r\npub struct DocumentExtractor;\r\n\r\nimpl DocumentExtractor {\r\n    pub fn new() -> Self {\r\n        Self\r\n    }\r\n    \r\n    pub async fn extract_content(&self, path: &Path) -> Result<String> {\r\n        let extension = path.extension()\r\n            .and_then(|ext| ext.to_str())\r\n            .unwrap_or(\"\")\r\n            .to_lowercase();\r\n        \r\n        match extension.as_str() {\r\n            \"docx\" => self.extract_docx(path).await,\r\n            \"pdf\" => self.extract_pdf(path).await,\r\n            \"xlsx\" => self.extract_xlsx(path).await,\r\n            \"pptx\" => self.extract_pptx(path).await,\r\n            \"txt\" | \"md\" | \"rst\" | \"log\" => {\r\n                tokio::fs::read_to_string(path).await.map_err(Into::into)\r\n            }\r\n            _ => {\r\n                // Try to read as text, fall back to binary indicator\r\n                match tokio::fs::read_to_string(path).await {\r\n                    Ok(content) => Ok(content),\r\n                    Err(_) => Ok(format!(\"[Binary file: {}]\", path.display())),\r\n                }\r\n            }\r\n        }\r\n    }\r\n    \r\n    async fn extract_docx(&self, path: &Path) -> Result<String> {\r\n        // DOCX extraction temporarily disabled due to dependency issues\r\n        log::info!(\"DOCX extraction not yet implemented for: {}\", path.display());\r\n        Ok(format!(\"[DOCX file: {} - content extraction not implemented]\", path.display()))\r\n    }\r\n    \r\n    async fn extract_pdf(&self, path: &Path) -> Result<String> {\r\n        // Try lopdf first\r\n        match self.extract_pdf_with_lopdf(path).await {\r\n            Ok(content) if !content.trim().is_empty() => Ok(content),\r\n            _ => {\r\n                // Fallback to pdftotext\r\n                self.try_pdftotext(path).await\r\n            }\r\n        }\r\n    }\r\n    \r\n    async fn extract_pdf_with_lopdf(&self, path: &Path) -> Result<String> {\r\n        // lopdf temporarily disabled due to compilation issues\r\n        log::info!(\"PDF extraction with lopdf not available for: {}\", path.display());\r\n        Ok(String::new())\r\n    }\r\n    \r\n    async fn try_pdftotext(&self, path: &Path) -> Result<String> {\r\n        let output = Command::new(\"pdftotext\")\r\n            .args(&[path.to_str().unwrap(), \"-\"]) // Read from file, write to stdout\r\n            .output()?;\r\n        \r\n        if output.status.success() {\r\n            Ok(String::from_utf8_lossy(&output.stdout).to_string())\r\n        } else {\r\n            log::warn!(\"pdftotext failed for {}: {}\", path.display(), String::from_utf8_lossy(&output.stderr));\r\n            Ok(format!(\"[Failed to extract PDF content: pdftotext not available]\"))\r\n        }\r\n    }\r\n    \r\n    async fn extract_xlsx(&self, path: &Path) -> Result<String> {\r\n        // XLSX extraction temporarily simplified\r\n        log::info!(\"XLSX extraction not yet fully implemented for: {}\", path.display());\r\n        Ok(format!(\"[XLSX file: {} - content extraction not implemented]\", path.display()))\r\n    }\r\n    \r\n    async fn extract_pptx(&self, path: &Path) -> Result<String> {\r\n        // PowerPoint extraction is complex, for now return a placeholder\r\n        log::info!(\"PowerPoint extraction not yet implemented for: {}\", path.display());\r\n        Ok(format!(\"[PowerPoint file: {} - content extraction not implemented]\", path.display()))\r\n    }\r\n}\r\n",
  "src\\search\\mod.rs": "pub mod engine;\r\n",
  "src\\platform.rs": "use std::path::Path;\r\nuse anyhow::Result;\r\n\r\nuse crate::file_discovery::FileDiscovery;\r\nuse crate::analysis::duplicates::{SmartDuplicateDetector, DuplicateResults};\r\nuse crate::search::engine::{RipgrepSearchEngine, SearchResult};\r\n\r\npub struct FerretPlatform {\r\n    file_discovery: FileDiscovery,\r\n    duplicate_detector: SmartDuplicateDetector,\r\n    search_engine: Option<RipgrepSearchEngine>,\r\n}\r\n\r\nimpl FerretPlatform {\r\n    pub fn new() -> Result<Self> {\r\n        Ok(Self {\r\n            file_discovery: FileDiscovery::new(),\r\n            duplicate_detector: SmartDuplicateDetector::new(),\r\n            search_engine: None,\r\n        })\r\n    }\r\n    \r\n    pub fn set_search_engine(&mut self, engine: RipgrepSearchEngine) {\r\n        self.search_engine = Some(engine);\r\n    }\r\n    \r\n    pub async fn analyze_directory(&mut self, path: &Path) -> Result<AnalysisResults> {\r\n        let file_groups = self.file_discovery.discover_files(path).await?;\r\n        let duplicate_results = self.duplicate_detector.detect_duplicates(&file_groups).await?;\r\n        \r\n        Ok(AnalysisResults {\r\n            total_files: file_groups.iter().map(|g| g.variants.len()).sum(),\r\n            total_groups: file_groups.len(),\r\n            duplicate_results,\r\n        })\r\n    }\r\n    \r\n    pub async fn search(&mut self, query: &str, path: &Path, limit: usize) -> Result<Vec<SearchResult>> {\r\n        if let Some(ref engine) = self.search_engine {\r\n            engine.search(query, limit).await\r\n        } else {\r\n            // Fallback to ripgrep integration\r\n            let integration = crate::search::engine::RipgrepIntegration;\r\n            let mut results = integration.search_with_ripgrep(query, path).await?;\r\n            \r\n            // Apply limit\r\n            results.truncate(limit);\r\n            Ok(results)\r\n        }\r\n    }\r\n    \r\n    pub async fn index_directory(&mut self, path: &Path, index_path: &Path) -> Result<()> {\r\n        let mut engine = RipgrepSearchEngine::new(index_path)?;\r\n        \r\n        // Discover all files\r\n        let file_groups = self.file_discovery.discover_files(path).await?;\r\n        \r\n        for group in file_groups {\r\n            for file in group.variants {\r\n                let metadata = crate::search::engine::FileMetadata {\r\n                    size: std::fs::metadata(&file)?.len(),\r\n                    modified: std::fs::metadata(&file)?.modified()?,\r\n                    is_binary: false, // Would need proper binary detection\r\n                };\r\n                \r\n                engine.index_file(&file, &metadata).await?;\r\n            }\r\n        }\r\n        \r\n        engine.commit()?;\r\n        self.search_engine = Some(engine);\r\n        \r\n        Ok(())\r\n    }\r\n}\r\n\r\n#[derive(Debug, serde::Serialize)]\r\npub struct AnalysisResults {\r\n    pub total_files: usize,\r\n    pub total_groups: usize,\r\n    pub duplicate_results: DuplicateResults,\r\n}\r\n\r\n\r\n",
  "src\\extraction\\mod.rs": "pub mod document;\r\n",
  "src\\file_discovery.rs": "use std::collections::HashMap;\r\nuse std::path::{Path, PathBuf};\r\nuse std::process::Command;\r\nuse anyhow::Result;\r\nuse fuzzy_matcher::skim::SkimMatcherV2;\r\nuse fuzzy_matcher::FuzzyMatcher;\r\n\r\npub struct FileDiscovery {\r\n    grouper: SmartGrouper,\r\n    fd_integration: FdIntegration,\r\n}\r\n\r\nimpl FileDiscovery {\r\n    pub fn new() -> Self {\r\n        Self {\r\n            grouper: SmartGrouper::new(),\r\n            fd_integration: FdIntegration,\r\n        }\r\n    }\r\n    \r\n    pub async fn discover_files(&self, root_path: &Path) -> Result<Vec<FileGroup>> {\r\n        // Use fd for fast file discovery\r\n        let files = self.fd_integration.find_files(\"*\", root_path).await?;\r\n        \r\n        // Group files by normalized names\r\n        let groups = self.grouper.group_files(files);\r\n        \r\n        Ok(groups)\r\n    }\r\n}\r\n\r\npub struct SmartGrouper {\r\n    matcher: SkimMatcherV2,\r\n    threshold: i64,\r\n}\r\n\r\nimpl SmartGrouper {\r\n    pub fn new() -> Self {\r\n        Self {\r\n            matcher: SkimMatcherV2::default(),\r\n            threshold: 60, // Fuzzy matching threshold\r\n        }\r\n    }\r\n    \r\n    pub fn group_files(&self, files: Vec<PathBuf>) -> Vec<FileGroup> {\r\n        let mut groups: HashMap<String, FileGroup> = HashMap::new();\r\n        \r\n        for file in files {\r\n            let normalized = self.normalize_filename(&file);\r\n            \r\n            // Try to find existing group with similar name\r\n            let mut found_group = false;\r\n            for (canonical_name, group) in &mut groups {\r\n                if let Some(score) = self.matcher.fuzzy_match(&normalized, canonical_name) {\r\n                    if score >= self.threshold {\r\n                        group.variants.push(file.clone());\r\n                        found_group = true;\r\n                        break;\r\n                    }\r\n                }\r\n            }\r\n            \r\n            if !found_group {\r\n                groups.insert(normalized.clone(), FileGroup {\r\n                    canonical_name: normalized,\r\n                    variants: vec![file],\r\n                });\r\n            }\r\n        }\r\n        \r\n        groups.into_values().collect()\r\n    }\r\n    \r\n    fn normalize_filename(&self, path: &Path) -> String {\r\n        let stem = path.file_stem()\r\n            .and_then(|s| s.to_str())\r\n            .unwrap_or(\"\");\r\n        \r\n        // For now, use a simpler approach that keeps more of the filename\r\n        // Remove common suffixes like numbers, but keep the base name\r\n        let normalized = stem\r\n            .to_lowercase()\r\n            .replace(\"_\", \" \")\r\n            .replace(\"-\", \" \");\r\n        \r\n        // Remove trailing numbers and common patterns\r\n        let cleaned = regex::Regex::new(r\"\\s*(v\\d+|copy|backup|final|draft|\\d+)$\")\r\n            .unwrap()\r\n            .replace(&normalized, \"\")\r\n            .trim()\r\n            .to_string();\r\n        \r\n        if cleaned.is_empty() {\r\n            stem.to_lowercase()\r\n        } else {\r\n            cleaned\r\n        }\r\n    }\r\n    \r\n}\r\n\r\npub struct FdIntegration;\r\n\r\nimpl FdIntegration {\r\n    pub async fn find_files(&self, pattern: &str, root_path: &Path) -> Result<Vec<PathBuf>> {\r\n        // Try fd first - it's 10x faster than walkdir\r\n        if self.is_fd_available().await {\r\n            self.find_files_with_fd(pattern, root_path).await\r\n        } else {\r\n            // Fallback to walkdir\r\n            log::warn!(\"fd not available, falling back to walkdir\");\r\n            self.fallback_find_files(pattern, root_path).await\r\n        }\r\n    }\r\n    \r\n    async fn is_fd_available(&self) -> bool {\r\n        Command::new(\"fd\")\r\n            .arg(\"--version\")\r\n            .output()\r\n            .map(|output| output.status.success())\r\n            .unwrap_or(false)\r\n    }\r\n    \r\n    async fn find_files_with_fd(&self, pattern: &str, root_path: &Path) -> Result<Vec<PathBuf>> {\r\n        let output = Command::new(\"fd\")\r\n            .args(&[\r\n                \"--type\", \"f\",\r\n                \"--hidden\",\r\n                \"--no-ignore\",\r\n                pattern,\r\n                root_path.to_str().unwrap()\r\n            ])\r\n            .output()?;\r\n        \r\n        if !output.status.success() {\r\n            return Err(anyhow::anyhow!(\"fd command failed\"));\r\n        }\r\n        \r\n        let output_str = String::from_utf8(output.stdout)?;\r\n        let files: Vec<PathBuf> = output_str\r\n            .lines()\r\n            .map(|line| PathBuf::from(line.trim()))\r\n            .collect();\r\n        \r\n        Ok(files)\r\n    }\r\n    \r\n    async fn fallback_find_files(&self, pattern: &str, root_path: &Path) -> Result<Vec<PathBuf>> {\r\n        use walkdir::WalkDir;\r\n        use regex::Regex;\r\n        \r\n        // Handle special patterns\r\n        let regex_pattern = match pattern {\r\n            \"*\" => \".*\",  // Convert glob * to regex .*\r\n            _ => pattern,\r\n        };\r\n        \r\n        let regex = Regex::new(regex_pattern)?;\r\n        let mut files = Vec::new();\r\n        \r\n        for entry in WalkDir::new(root_path)\r\n            .into_iter()\r\n            .filter_map(Result::ok)\r\n        {\r\n            if entry.file_type().is_file() {\r\n                let path = entry.path();\r\n                if let Some(filename) = path.file_name().and_then(|s| s.to_str()) {\r\n                    if regex.is_match(filename) {\r\n                        files.push(path.to_path_buf());\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        \r\n        Ok(files)\r\n    }\r\n}\r\n\r\npub struct FileGroup {\r\n    pub canonical_name: String,\r\n    pub variants: Vec<PathBuf>,\r\n}\r\n\r\nimpl FileGroup {\r\n    \r\n    pub fn is_potential_duplicate(&self) -> bool {\r\n        self.variants.len() > 1\r\n    }\r\n}\r\n"
}