{
  "src\\search\\engine.rs": {
    "content": "use std::path::{Path, PathBuf};\r\nuse anyhow::Result;\r\n// Future enhancement: Tantivy integration for advanced full-text search\r\n// Tantivy would provide more sophisticated search capabilities like:\r\n// - BM25 relevance scoring\r\n// - Boolean queries and phrase search\r\n// - Faceted search and filtering\r\n// - Distributed search across multiple indexes\r\n// use tantivy::{\r\n//     collector::TopDocs,\r\n//     directory::MmapDirectory,\r\n//     query::QueryParser,\r\n//     schema::{Schema, Field, STORED, TEXT},\r\n//     Index, IndexReader, IndexWriter, ReloadPolicy, Term,\r\n// };\r\nuse ignore::WalkBuilder;  // Fast file traversal with .gitignore support\r\nuse crate::extraction::document::DocumentExtractor;  // For extracting text from documents\r\nuse serde::{Deserialize, Serialize};  // For JSON serialization of the index\r\n\r\n/// Represents a single file entry in our search index\r\n/// Contains all metadata needed for fast searching without re-reading files\r\n#[derive(Debug, Serialize, Deserialize)]\r\nstruct IndexEntry {\r\n    /// Full text content of the file (extracted from documents or raw text)\r\n    content: String,\r\n    /// File size in bytes - used for relevance scoring and filtering\r\n    file_size: u64,\r\n    /// Last modified timestamp (Unix epoch seconds) - used for freshness scoring\r\n    modified: u64,\r\n    /// File extension/type - used for filtering and relevance scoring\r\n    file_type: String,\r\n}\r\n\r\n/// Persistent search engine that maintains a JSON-based index\r\n/// This allows for fast repeated searches without re-scanning the filesystem\r\n/// The index stores file content and metadata for instant retrieval\r\npub struct RipgrepSearchEngine {\r\n    /// Directory where the search index files are stored\r\n    index_path: std::path::PathBuf,\r\n}\r\n\r\n/// Real-time search integration using ripgrep-like functionality\r\n/// This performs live filesystem searches without requiring pre-built indexes\r\n/// Used as fallback when no index exists or for one-off searches\r\npub struct RipgrepIntegration;\r\n\r\nimpl RipgrepIntegration {\r\n    /// Performs a real-time search across files in the given directory\r\n    /// This method scans files on-demand without requiring a pre-built index\r\n    /// \r\n    /// # Arguments\r\n    /// * `pattern` - The search term to look for (case-insensitive)\r\n    /// * `path` - Directory to search within\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<Vec<SearchResult>>` - List of search results with context and metadata\r\n    pub async fn search_with_ripgrep(&self, pattern: &str, path: &Path) -> Result<Vec<SearchResult>> {\r\n        let extractor = DocumentExtractor::new();\r\n        // Map file paths to their matching lines for efficient processing\r\n        let mut file_results: std::collections::HashMap<PathBuf, Vec<(u64, String)>> = std::collections::HashMap::new();\r\n        \r\n        // First pass: collect all matches per file\r\n        // We use WalkBuilder (from ignore crate) for efficient filesystem traversal\r\n        // This respects .gitignore and other ignore files automatically\r\n        for entry in WalkBuilder::new(path).build() {\r\n            let entry = entry?;\r\n            // Skip directories and non-files (symlinks, etc.)\r\n            if !entry.file_type().map_or(false, |ft| ft.is_file()) {\r\n                continue;\r\n            }\r\n            \r\n            let file_path = entry.path();\r\n            let file_extension = file_path.extension()\r\n                .and_then(|s| s.to_str())\r\n                .unwrap_or(\"\")\r\n                .to_lowercase();\r\n            \r\n            // Handle document files (PDF, DOCX, etc.) that need content extraction\r\n            // These files can't be read as plain text, so we extract their content first\r\n            if matches!(file_extension.as_str(), \"pdf\" | \"docx\" | \"xlsx\" | \"pptx\") {\r\n                match extractor.extract_content(file_path).await {\r\n                    Ok(content) => {\r\n                        // Search in the extracted text content\r\n                        let lines: Vec<&str> = content.lines().collect();\r\n                        for (line_num, line) in lines.iter().enumerate() {\r\n                            // Case-insensitive substring matching\r\n                            if line.to_lowercase().contains(&pattern.to_lowercase()) {\r\n                                file_results.entry(file_path.to_path_buf())\r\n                                    .or_insert_with(Vec::new)\r\n                                    .push((line_num as u64 + 1, line.to_string()));\r\n                            }\r\n                        }\r\n                    }\r\n                    Err(e) => {\r\n                        // Log extraction failures but continue processing other files\r\n                        log::warn!(\"Failed to extract content from {}: {}\", file_path.display(), e);\r\n                        continue;\r\n                    }\r\n                }\r\n            } else {\r\n                // For plain text files, read directly without extraction\r\n                if let Ok(content) = std::fs::read_to_string(file_path) {\r\n                    let lines: Vec<&str> = content.lines().collect();\r\n                    for (line_num, line) in lines.iter().enumerate() {\r\n                        // Case-insensitive substring matching\r\n                        if line.to_lowercase().contains(&pattern.to_lowercase()) {\r\n                            file_results.entry(file_path.to_path_buf())\r\n                                .or_insert_with(Vec::new)\r\n                                .push((line_num as u64 + 1, line.to_string()));\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        \r\n        // Second pass: create rich search results with context\r\n        // Now we process each file's matches to create SearchResult objects with context\r\n        let mut results = Vec::new();\r\n        for (file_path, matches) in file_results {\r\n            // Get file metadata for the search result\r\n            let file_size = std::fs::metadata(&file_path).map(|m| m.len()).unwrap_or(0);\r\n            let file_type = file_path.extension()\r\n                .and_then(|s| s.to_str())\r\n                .unwrap_or(\"unknown\")\r\n                .to_string();\r\n            let match_count = matches.len();\r\n            \r\n            // Re-read file content to provide context around matches\r\n            // For document files, we need to extract content again\r\n            // For text files, we can read directly\r\n            let file_content = if matches!(file_type.as_str(), \"pdf\" | \"docx\" | \"xlsx\" | \"pptx\") {\r\n                extractor.extract_content(&file_path).await.unwrap_or_default()\r\n            } else {\r\n                std::fs::read_to_string(&file_path).unwrap_or_default()\r\n            };\r\n            \r\n            let lines: Vec<&str> = file_content.lines().collect();\r\n            \r\n            // Group nearby matches to avoid duplicate results\r\n            // If multiple matches are close together, we only show one result\r\n            // to avoid cluttering the output with overlapping context\r\n            let mut processed_lines = std::collections::HashSet::new();\r\n            \r\n            for (line_num, line_content) in &matches {\r\n                // Skip if we've already processed a nearby line\r\n                if processed_lines.contains(line_num) {\r\n                    continue;\r\n                }\r\n                \r\n                // Calculate context lines (3 lines before and after the match)\r\n                // This helps users understand the match in context\r\n                let context_before: Vec<String> = lines\r\n                    .iter()\r\n                    .skip((line_num.saturating_sub(4)) as usize)\r\n                    .take(3)\r\n                    .map(|s| s.to_string())\r\n                    .collect();\r\n                \r\n                let context_after: Vec<String> = lines\r\n                    .iter()\r\n                    .skip((line_num + 1) as usize)\r\n                    .take(3)\r\n                    .map(|s| s.to_string())\r\n                    .collect();\r\n                \r\n                // Calculate relevance score based on match quality and file properties\r\n                let score = self.calculate_relevance_score(line_content, pattern, &file_path);\r\n                \r\n                // Mark nearby lines as processed to avoid duplicates\r\n                // This prevents showing multiple results for the same context\r\n                for i in line_num.saturating_sub(2)..=line_num + 2 {\r\n                    processed_lines.insert(i);\r\n                }\r\n                \r\n                // Create the search result with all context and metadata\r\n                results.push(SearchResult {\r\n                    path: file_path.clone(),\r\n                    score,\r\n                    snippet: line_content.clone(),\r\n                    line_number: Some(*line_num),\r\n                    context_before,\r\n                    context_after,\r\n                    match_count,\r\n                    file_size,\r\n                    file_type: file_type.clone(),\r\n                });\r\n            }\r\n        }\r\n        \r\n        // Sort by relevance score (highest first) to show most relevant results first\r\n        results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap_or(std::cmp::Ordering::Equal));\r\n        \r\n        Ok(results)\r\n    }\r\n    \r\n    /// Calculates a relevance score for a search match\r\n    /// Higher scores indicate more relevant matches that should appear first\r\n    /// \r\n    /// # Arguments\r\n    /// * `line` - The line containing the match\r\n    /// * `pattern` - The search pattern that was matched\r\n    /// * `file_path` - Path to the file containing the match\r\n    /// \r\n    /// # Returns\r\n    /// * `f32` - Relevance score (higher = more relevant)\r\n    fn calculate_relevance_score(&self, line: &str, pattern: &str, file_path: &Path) -> f32 {\r\n        let mut score = 1.0; // Base score for any match\r\n        \r\n        // Boost score for exact matches (case-insensitive)\r\n        if line.to_lowercase().contains(&pattern.to_lowercase()) {\r\n            score += 2.0;\r\n        }\r\n        \r\n        // Boost score for multiple occurrences in the same line\r\n        // This indicates the line is highly relevant to the search term\r\n        let occurrences = line.to_lowercase().matches(&pattern.to_lowercase()).count();\r\n        score += occurrences as f32 * 0.5;\r\n        \r\n        // Boost score for filename matches - these are often very relevant\r\n        // If the search term appears in the filename, it's likely very important\r\n        if let Some(filename) = file_path.file_name().and_then(|n| n.to_str()) {\r\n            if filename.to_lowercase().contains(&pattern.to_lowercase()) {\r\n                score += 1.5;\r\n            }\r\n        }\r\n        \r\n        // Boost score for shorter lines (more precise matches)\r\n        // Shorter lines with matches are often more focused and relevant\r\n        if line.len() < 100 {\r\n            score += 0.5;\r\n        }\r\n        \r\n        score\r\n    }\r\n}\r\n\r\n\r\nimpl RipgrepSearchEngine {\r\n    /// Creates a new search engine with the specified index directory\r\n    /// The index directory is where all search index files will be stored\r\n    /// \r\n    /// # Arguments\r\n    /// * `index_path` - Directory where the search index will be stored\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<Self>` - The new search engine instance\r\n    pub fn new(index_path: &Path) -> Result<Self> {\r\n        Ok(Self {\r\n            index_path: index_path.to_path_buf(),\r\n        })\r\n    }\r\n    \r\n    /// Indexes a single file for fast searching\r\n    /// This extracts content from the file and stores it with metadata in the index\r\n    /// \r\n    /// # Arguments\r\n    /// * `path` - Path to the file to index\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<()>` - Success or error\r\n    pub async fn index_file(&mut self, path: &Path) -> Result<()> {\r\n        use std::collections::HashMap;\r\n        use serde_json;\r\n        \r\n        // Create index directory if it doesn't exist\r\n        std::fs::create_dir_all(&self.index_path)?;\r\n        \r\n        // Extract content using DocumentExtractor\r\n        // This handles different file types (PDF, DOCX, etc.) automatically\r\n        let extractor = DocumentExtractor::new();\r\n        let content = match extractor.extract_content(path).await {\r\n            Ok(content) => content,\r\n            Err(_) => return Ok(()), // Skip files that can't be processed\r\n        };\r\n        \r\n        // Get file metadata for the index entry\r\n        let metadata = std::fs::metadata(path)?;\r\n        let file_size = metadata.len();\r\n        let modified = metadata.modified()?;\r\n        let file_type = path.extension()\r\n            .and_then(|s| s.to_str())\r\n            .unwrap_or(\"unknown\")\r\n            .to_string();\r\n        \r\n        // Create enhanced index entry with all metadata\r\n        let index_entry = IndexEntry {\r\n            content,\r\n            file_size,\r\n            modified: modified.duration_since(std::time::UNIX_EPOCH).unwrap_or_default().as_secs(),\r\n            file_type,\r\n        };\r\n        \r\n        // Load existing index or create new one\r\n        let index_file = self.index_path.join(\"ferret_index.json\");\r\n        let mut index: HashMap<String, IndexEntry> = if index_file.exists() {\r\n            let data = tokio::fs::read_to_string(&index_file).await?;\r\n            serde_json::from_str(&data).unwrap_or_default()\r\n        } else {\r\n            HashMap::new()\r\n        };\r\n        \r\n        // Add file to index (overwrites if already exists)\r\n        index.insert(path.to_string_lossy().to_string(), index_entry);\r\n        \r\n        // Write updated index back to disk\r\n        let json = serde_json::to_string_pretty(&index)?;\r\n        tokio::fs::write(&index_file, json).await?;\r\n        \r\n        Ok(())\r\n    }\r\n    \r\n    /// Searches through the pre-built index for fast results\r\n    /// This method is much faster than real-time search as it doesn't read files from disk\r\n    /// \r\n    /// # Arguments\r\n    /// * `query_str` - The search term to look for (case-insensitive)\r\n    /// * `limit` - Maximum number of results to return\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<Vec<SearchResult>>` - List of search results with context and metadata\r\n    pub async fn search(&self, query_str: &str, limit: usize) -> Result<Vec<SearchResult>> {\r\n        use std::collections::HashMap;\r\n        use serde_json;\r\n        \r\n        let index_file = self.index_path.join(\"ferret_index.json\");\r\n        if !index_file.exists() {\r\n            return Ok(Vec::new());\r\n        }\r\n        \r\n        // Load the entire index from disk\r\n        // This is fast because we only read one JSON file instead of many individual files\r\n        let data = tokio::fs::read_to_string(&index_file).await?;\r\n        let index: HashMap<String, IndexEntry> = serde_json::from_str(&data)?;\r\n        \r\n        let mut results = Vec::new();\r\n        let query_lower = query_str.to_lowercase();\r\n        \r\n        // Search through each indexed file\r\n        for (file_path_str, entry) in index {\r\n            let file_path = std::path::PathBuf::from(&file_path_str);\r\n            let lines: Vec<&str> = entry.content.lines().collect();\r\n            let mut matches = Vec::new();\r\n            \r\n            // Search in the pre-extracted content\r\n            for (line_num, line) in lines.iter().enumerate() {\r\n                if line.to_lowercase().contains(&query_lower) {\r\n                    matches.push((line_num as u64 + 1, line.to_string()));\r\n                }\r\n            }\r\n            \r\n            if !matches.is_empty() {\r\n                let match_count = matches.len();\r\n                \r\n                // Create search results for each match with context\r\n                for (line_num, line_content) in matches {\r\n                    // Calculate context lines (3 before and after)\r\n                    let context_before: Vec<String> = lines\r\n                        .iter()\r\n                        .skip((line_num.saturating_sub(4)) as usize)\r\n                        .take(3)\r\n                        .map(|s| s.to_string())\r\n                        .collect();\r\n                    \r\n                    let context_after: Vec<String> = lines\r\n                        .iter()\r\n                        .skip((line_num + 1) as usize)\r\n                        .take(3)\r\n                        .map(|s| s.to_string())\r\n                        .collect();\r\n                    \r\n                    // Calculate relevance score using the same algorithm as real-time search\r\n                    let score = self.calculate_relevance_score(&line_content, query_str, &file_path);\r\n                    \r\n                    // Create search result with all metadata from the index\r\n                    results.push(SearchResult {\r\n                        path: file_path.clone(),\r\n                        score,\r\n                        snippet: line_content.clone(),\r\n                        line_number: Some(line_num),\r\n                        context_before,\r\n                        context_after,\r\n                        match_count,\r\n                        file_size: entry.file_size,\r\n                        file_type: entry.file_type.clone(),\r\n                    });\r\n                }\r\n            }\r\n        }\r\n        \r\n        // Sort by relevance score and apply limit\r\n        results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap_or(std::cmp::Ordering::Equal));\r\n        results.truncate(limit);\r\n        \r\n        Ok(results)\r\n    }\r\n    \r\n    fn calculate_relevance_score(&self, line: &str, pattern: &str, file_path: &std::path::Path) -> f32 {\r\n        let mut score = 1.0;\r\n        \r\n        // Boost score for exact matches\r\n        if line.to_lowercase().contains(&pattern.to_lowercase()) {\r\n            score += 2.0;\r\n        }\r\n        \r\n        // Boost score for multiple occurrences in the line\r\n        let occurrences = line.to_lowercase().matches(&pattern.to_lowercase()).count();\r\n        score += occurrences as f32 * 0.5;\r\n        \r\n        // Boost score for filename matches\r\n        if let Some(filename) = file_path.file_name().and_then(|n| n.to_str()) {\r\n            if filename.to_lowercase().contains(&pattern.to_lowercase()) {\r\n                score += 1.5;\r\n            }\r\n        }\r\n        \r\n        // Boost score for shorter lines (more precise matches)\r\n        if line.len() < 100 {\r\n            score += 0.5;\r\n        }\r\n        \r\n        score\r\n    }\r\n    \r\n    pub fn commit(&mut self) -> Result<()> {\r\n        // Index is automatically committed on each file addition\r\n        Ok(())\r\n    }\r\n}\r\n\r\n/// Represents a single search result with context and metadata\r\n/// This structure contains all the information needed to display a search result\r\n/// to the user, including the matching text, context, and file information\r\n#[derive(Debug, Clone)]\r\npub struct SearchResult {\r\n    /// Path to the file containing the match\r\n    pub path: PathBuf,\r\n    /// Relevance score for this result (higher = more relevant)\r\n    pub score: f32,\r\n    /// The actual line of text that matched the search query\r\n    pub snippet: String,\r\n    /// Line number where the match occurred (if available)\r\n    pub line_number: Option<u64>,\r\n    /// Lines before the match for context (typically 3 lines)\r\n    pub context_before: Vec<String>,\r\n    /// Lines after the match for context (typically 3 lines)\r\n    pub context_after: Vec<String>,\r\n    /// Total number of matches found in this file\r\n    pub match_count: usize,\r\n    /// Size of the file in bytes\r\n    pub file_size: u64,\r\n    /// File extension/type (e.g., \"txt\", \"pdf\", \"docx\")\r\n    pub file_type: String,\r\n}\r\n\r\n",
    "file_size": 19770,
    "modified": 1756990842,
    "file_type": "rs"
  },
  "src\\platform.rs": {
    "content": "use std::path::Path;\r\nuse anyhow::Result;\r\n\r\nuse crate::file_discovery::FileDiscovery;  // For discovering and grouping files\r\nuse crate::analysis::duplicates::{SmartDuplicateDetector, DuplicateResults};  // For duplicate detection\r\nuse crate::search::engine::{RipgrepSearchEngine, SearchResult};  // For search functionality\r\n\r\n/// Main platform that orchestrates all FERRET functionality\r\n/// This is the central hub that coordinates file discovery, duplicate detection, and search\r\n/// It provides a unified interface for all the tool's capabilities\r\npub struct FerretPlatform {\r\n    /// Handles file discovery and intelligent grouping\r\n    file_discovery: FileDiscovery,\r\n    /// Detects exact duplicates using SHA-256 hashing\r\n    duplicate_detector: SmartDuplicateDetector,\r\n    /// Optional search engine for indexed searches (faster than real-time)\r\n    search_engine: Option<RipgrepSearchEngine>,\r\n}\r\n\r\nimpl FerretPlatform {\r\n    /// Creates a new FerretPlatform instance with all components initialized\r\n    /// This is the main entry point for creating a FERRET platform\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<Self>` - The new platform instance\r\n    pub fn new() -> Result<Self> {\r\n        Ok(Self {\r\n            file_discovery: FileDiscovery::new(),\r\n            duplicate_detector: SmartDuplicateDetector::new(),\r\n            search_engine: None,\r\n        })\r\n    }\r\n    \r\n    /// Sets the search engine for indexed searches\r\n    /// This enables faster searches when an index is available\r\n    /// \r\n    /// # Arguments\r\n    /// * `engine` - The search engine to use for indexed searches\r\n    pub fn set_search_engine(&mut self, engine: RipgrepSearchEngine) {\r\n        self.search_engine = Some(engine);\r\n    }\r\n    \r\n    /// Analyzes a directory for duplicates and file organization\r\n    /// This is the main analysis function that discovers files and finds duplicates\r\n    /// \r\n    /// # Arguments\r\n    /// * `path` - Directory to analyze\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<AnalysisResults>` - Analysis results including duplicate statistics\r\n    pub async fn analyze_directory(&mut self, path: &Path) -> Result<AnalysisResults> {\r\n        // Step 1: Discover and group files by similarity\r\n        let file_groups = self.file_discovery.discover_files(path).await?;\r\n        \r\n        // Step 2: Detect exact duplicates within similar file groups\r\n        let duplicate_results = self.duplicate_detector.detect_duplicates(&file_groups).await?;\r\n        \r\n        // Step 3: Calculate statistics and return results\r\n        Ok(AnalysisResults {\r\n            total_files: file_groups.iter().map(|g| g.variants.len()).sum(),\r\n            total_groups: file_groups.len(),\r\n            duplicate_results,\r\n        })\r\n    }\r\n    \r\n    /// Searches for text within files in a directory\r\n    /// This method provides both indexed and real-time search capabilities\r\n    /// \r\n    /// # Arguments\r\n    /// * `query` - Search term to look for\r\n    /// * `path` - Directory to search within\r\n    /// * `limit` - Maximum number of results to return\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<Vec<SearchResult>>` - List of search results with context\r\n    pub async fn search(&mut self, query: &str, path: &Path, limit: usize) -> Result<Vec<SearchResult>> {\r\n        if let Some(ref engine) = self.search_engine {\r\n            // Use indexed search if available (much faster)\r\n            engine.search(query, limit).await\r\n        } else {\r\n            // Fallback to real-time search using ripgrep integration\r\n            let integration = crate::search::engine::RipgrepIntegration;\r\n            let mut results = integration.search_with_ripgrep(query, path).await?;\r\n            \r\n            // Apply limit to results\r\n            results.truncate(limit);\r\n            Ok(results)\r\n        }\r\n    }\r\n    \r\n    /// Indexes a directory for fast searching\r\n    /// This creates a persistent index that enables much faster subsequent searches\r\n    /// \r\n    /// # Arguments\r\n    /// * `path` - Directory to index\r\n    /// * `index_path` - Where to store the index files\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<()>` - Success or error\r\n    pub async fn index_directory(&mut self, path: &Path, index_path: &Path) -> Result<()> {\r\n        // Create a new search engine for this index\r\n        let mut engine = RipgrepSearchEngine::new(index_path)?;\r\n        \r\n        // Discover all files in the directory\r\n        let file_groups = self.file_discovery.discover_files(path).await?;\r\n        \r\n        // Index each file for fast searching\r\n        for group in file_groups {\r\n            for file in group.variants {\r\n                engine.index_file(&file).await?;\r\n            }\r\n        }\r\n        \r\n        // Commit the index and set it as the active search engine\r\n        engine.commit()?;\r\n        self.search_engine = Some(engine);\r\n        \r\n        Ok(())\r\n    }\r\n}\r\n\r\n/// Results of directory analysis\r\n/// Contains statistics about files found and duplicates detected\r\n#[derive(Debug, serde::Serialize)]\r\npub struct AnalysisResults {\r\n    /// Total number of files discovered\r\n    pub total_files: usize,\r\n    /// Total number of file groups (similar files grouped together)\r\n    pub total_groups: usize,\r\n    /// Detailed duplicate detection results\r\n    pub duplicate_results: DuplicateResults,\r\n}\r\n\r\n\r\n",
    "file_size": 5357,
    "modified": 1756990877,
    "file_type": "rs"
  },
  "src\\extraction\\mod.rs": {
    "content": "pub mod document;\r\n",
    "file_size": 19,
    "modified": 1756944722,
    "file_type": "rs"
  },
  "src\\file_discovery.rs": {
    "content": "use std::collections::HashMap;\r\nuse std::path::{Path, PathBuf};\r\nuse std::process::Command;\r\nuse anyhow::Result;\r\nuse fuzzy_matcher::skim::SkimMatcherV2;\r\nuse fuzzy_matcher::FuzzyMatcher;\r\n\r\n/// Main file discovery system that finds and groups files for analysis\r\n/// This is a critical component that enables intelligent file grouping and duplicate detection\r\n/// It combines fast file system traversal with smart grouping algorithms\r\npub struct FileDiscovery {\r\n    /// Groups files by similarity using fuzzy matching\r\n    grouper: SmartGrouper,\r\n    /// Integrates with the `fd` command for fast file discovery\r\n    fd_integration: FdIntegration,\r\n}\r\n\r\nimpl FileDiscovery {\r\n    /// Creates a new FileDiscovery instance with default settings\r\n    pub fn new() -> Self {\r\n        Self {\r\n            grouper: SmartGrouper::new(),\r\n            fd_integration: FdIntegration,\r\n        }\r\n    }\r\n    \r\n    /// Discovers all files in a directory and groups them by similarity\r\n    /// This is the main entry point for file discovery and grouping\r\n    /// \r\n    /// # Arguments\r\n    /// * `root_path` - Directory to search for files\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<Vec<FileGroup>>` - Groups of similar files\r\n    pub async fn discover_files(&self, root_path: &Path) -> Result<Vec<FileGroup>> {\r\n        // Use fd for fast file discovery - much faster than walkdir for large directories\r\n        let files = self.fd_integration.find_files(\"*\", root_path).await?;\r\n        \r\n        // Group files by normalized names using fuzzy matching\r\n        // This groups files that are likely duplicates or variants\r\n        let groups = self.grouper.group_files(files);\r\n        \r\n        Ok(groups)\r\n    }\r\n}\r\n\r\n/// Groups files by similarity using fuzzy matching algorithms\r\n/// This is critical for identifying potential duplicates and file variants\r\n/// It normalizes filenames and groups files that are likely related\r\npub struct SmartGrouper {\r\n    /// Fuzzy matcher for calculating string similarity\r\n    matcher: SkimMatcherV2,\r\n    /// Minimum similarity score for grouping files (0-100)\r\n    threshold: i64,\r\n}\r\n\r\nimpl SmartGrouper {\r\n    /// Creates a new SmartGrouper with default settings\r\n    pub fn new() -> Self {\r\n        Self {\r\n            matcher: SkimMatcherV2::default(),\r\n            threshold: 60, // Fuzzy matching threshold - files must be 60% similar to group\r\n        }\r\n    }\r\n    \r\n    /// Groups files by normalized filename similarity\r\n    /// This is the core algorithm for identifying potential duplicates\r\n    /// \r\n    /// # Arguments\r\n    /// * `files` - List of file paths to group\r\n    /// \r\n    /// # Returns\r\n    /// * `Vec<FileGroup>` - Groups of similar files\r\n    pub fn group_files(&self, files: Vec<PathBuf>) -> Vec<FileGroup> {\r\n        let mut groups: HashMap<String, FileGroup> = HashMap::new();\r\n        \r\n        // Process each file and group by similarity\r\n        for file in files {\r\n            // Normalize the filename to remove common variations\r\n            // This handles things like \"file_v1.txt\" vs \"file_v2.txt\" vs \"file_final.txt\"\r\n            let normalized = self.normalize_filename(&file);\r\n            \r\n            // Try to find existing group with similar name using fuzzy matching\r\n            let mut found_group = false;\r\n            for (canonical_name, group) in &mut groups {\r\n                // Calculate similarity score between normalized names\r\n                if let Some(score) = self.matcher.fuzzy_match(&normalized, canonical_name) {\r\n                    if score >= self.threshold {\r\n                        // Files are similar enough to group together\r\n                        group.variants.push(file.clone());\r\n                        found_group = true;\r\n                        break;\r\n                    }\r\n                }\r\n            }\r\n            \r\n            // If no similar group found, create a new group\r\n            if !found_group {\r\n                groups.insert(normalized.clone(), FileGroup {\r\n                    canonical_name: normalized,\r\n                    variants: vec![file],\r\n                });\r\n            }\r\n        }\r\n        \r\n        // Convert HashMap values to Vec for return\r\n        groups.into_values().collect()\r\n    }\r\n    \r\n    /// Normalizes a filename to remove common variations and suffixes\r\n    /// This is critical for grouping similar files that have different naming conventions\r\n    /// \r\n    /// # Arguments\r\n    /// * `path` - Path to the file to normalize\r\n    /// \r\n    /// # Returns\r\n    /// * `String` - Normalized filename for grouping\r\n    fn normalize_filename(&self, path: &Path) -> String {\r\n        // Extract the filename without extension\r\n        let stem = path.file_stem()\r\n            .and_then(|s| s.to_str())\r\n            .unwrap_or(\"\");\r\n        \r\n        // Normalize the filename by:\r\n        // 1. Converting to lowercase for case-insensitive matching\r\n        // 2. Replacing separators with spaces for better word separation\r\n        let normalized = stem\r\n            .to_lowercase()\r\n            .replace(\"_\", \" \")  // Replace underscores with spaces\r\n            .replace(\"-\", \" \"); // Replace hyphens with spaces\r\n        \r\n        // Remove trailing numbers and common patterns that indicate file variants\r\n        // This regex removes version numbers, copy indicators, and common suffixes\r\n        let cleaned = regex::Regex::new(r\"\\s*(v\\d+|copy|backup|final|draft|\\d+)$\")\r\n            .unwrap()\r\n            .replace(&normalized, \"\")\r\n            .trim()\r\n            .to_string();\r\n        \r\n        // If cleaning removed everything, fall back to the original stem\r\n        if cleaned.is_empty() {\r\n            stem.to_lowercase()\r\n        } else {\r\n            cleaned\r\n        }\r\n    }\r\n    \r\n}\r\n\r\n/// Integrates with the `fd` command for fast file discovery\r\n/// `fd` is much faster than traditional directory traversal for large directories\r\n/// This provides a significant performance boost when scanning many files\r\npub struct FdIntegration;\r\n\r\nimpl FdIntegration {\r\n    /// Finds files using the `fd` command with fallback to walkdir\r\n    /// This is much faster than traditional directory traversal\r\n    /// \r\n    /// # Arguments\r\n    /// * `pattern` - File pattern to search for (e.g., \"*\" for all files)\r\n    /// * `root_path` - Directory to search in\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<Vec<PathBuf>>` - List of found file paths\r\n    pub async fn find_files(&self, pattern: &str, root_path: &Path) -> Result<Vec<PathBuf>> {\r\n        // Try fd first - it's 10x faster than walkdir for large directories\r\n        if self.is_fd_available().await {\r\n            self.find_files_with_fd(pattern, root_path).await\r\n        } else {\r\n            // Fallback to walkdir if fd is not available\r\n            // This ensures the tool works even without external dependencies\r\n            log::warn!(\"fd not available, falling back to walkdir\");\r\n            self.fallback_find_files(pattern, root_path).await\r\n        }\r\n    }\r\n    \r\n    /// Checks if the `fd` command is available on the system\r\n    /// This is used to determine whether to use fd or fallback to walkdir\r\n    async fn is_fd_available(&self) -> bool {\r\n        Command::new(\"fd\")\r\n            .arg(\"--version\")\r\n            .output()\r\n            .map(|output| output.status.success())\r\n            .unwrap_or(false)\r\n    }\r\n    \r\n    /// Uses the `fd` command to find files quickly\r\n    /// This is much faster than traditional directory traversal\r\n    /// \r\n    /// # Arguments\r\n    /// * `pattern` - File pattern to search for\r\n    /// * `root_path` - Directory to search in\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<Vec<PathBuf>>` - List of found file paths\r\n    async fn find_files_with_fd(&self, pattern: &str, root_path: &Path) -> Result<Vec<PathBuf>> {\r\n        let output = Command::new(\"fd\")\r\n            .args(&[\r\n                \"--type\", \"f\",      // Only files, not directories\r\n                \"--hidden\",         // Include hidden files\r\n                \"--no-ignore\",      // Don't respect .gitignore\r\n                pattern,\r\n                root_path.to_str().unwrap()\r\n            ])\r\n            .output()?;\r\n        \r\n        if !output.status.success() {\r\n            return Err(anyhow::anyhow!(\"fd command failed\"));\r\n        }\r\n        \r\n        // Parse fd output into PathBuf list\r\n        let output_str = String::from_utf8(output.stdout)?;\r\n        let files: Vec<PathBuf> = output_str\r\n            .lines()\r\n            .map(|line| PathBuf::from(line.trim()))\r\n            .collect();\r\n        \r\n        Ok(files)\r\n    }\r\n    \r\n    /// Fallback method using walkdir when fd is not available\r\n    /// This ensures the tool works even without external dependencies\r\n    /// \r\n    /// # Arguments\r\n    /// * `pattern` - File pattern to search for\r\n    /// * `root_path` - Directory to search in\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<Vec<PathBuf>>` - List of found file paths\r\n    async fn fallback_find_files(&self, pattern: &str, root_path: &Path) -> Result<Vec<PathBuf>> {\r\n        use walkdir::WalkDir;\r\n        use regex::Regex;\r\n        \r\n        // Convert glob patterns to regex patterns\r\n        let regex_pattern = match pattern {\r\n            \"*\" => \".*\",  // Convert glob * to regex .*\r\n            _ => pattern,\r\n        };\r\n        \r\n        let regex = Regex::new(regex_pattern)?;\r\n        let mut files = Vec::new();\r\n        \r\n        // Walk through directory tree and find matching files\r\n        for entry in WalkDir::new(root_path)\r\n            .into_iter()\r\n            .filter_map(Result::ok)  // Skip entries that caused errors\r\n        {\r\n            if entry.file_type().is_file() {\r\n                let path = entry.path();\r\n                if let Some(filename) = path.file_name().and_then(|s| s.to_str()) {\r\n                    if regex.is_match(filename) {\r\n                        files.push(path.to_path_buf());\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        \r\n        Ok(files)\r\n    }\r\n}\r\n\r\n/// Represents a group of files that are similar or potentially duplicates\r\n/// This is the core data structure for organizing files for analysis\r\npub struct FileGroup {\r\n    /// The canonical name used for grouping (normalized filename)\r\n    pub canonical_name: String,\r\n    /// List of file paths that belong to this group\r\n    pub variants: Vec<PathBuf>,\r\n}\r\n\r\nimpl FileGroup {\r\n    /// Checks if this group contains potential duplicates\r\n    /// A group with more than one file is considered to have potential duplicates\r\n    pub fn is_potential_duplicate(&self) -> bool {\r\n        self.variants.len() > 1\r\n    }\r\n}\r\n",
    "file_size": 10684,
    "modified": 1756990596,
    "file_type": "rs"
  },
  "src\\analysis\\mod.rs": {
    "content": "pub mod duplicates;\r\n",
    "file_size": 21,
    "modified": 1756987055,
    "file_type": "rs"
  },
  "src\\extraction\\document.rs": {
    "content": "use std::path::Path;\r\nuse anyhow::Result;\r\nuse std::process::Command;\r\n\r\n/// Extracts text content from various document formats\r\n/// This is a critical component that enables searching inside PDFs, Word docs, Excel files, etc.\r\n/// It uses external tools and basic parsing to extract text from binary document formats\r\npub struct DocumentExtractor;\r\n\r\nimpl DocumentExtractor {\r\n    /// Creates a new DocumentExtractor instance\r\n    pub fn new() -> Self {\r\n        Self\r\n    }\r\n    \r\n    /// Extracts text content from a file based on its extension\r\n    /// This is the main entry point for document content extraction\r\n    /// \r\n    /// # Arguments\r\n    /// * `path` - Path to the file to extract content from\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<String>` - Extracted text content or error\r\n    pub async fn extract_content(&self, path: &Path) -> Result<String> {\r\n        let extension = path.extension()\r\n            .and_then(|ext| ext.to_str())\r\n            .unwrap_or(\"\")\r\n            .to_lowercase();\r\n        \r\n        // Route to appropriate extraction method based on file type\r\n        match extension.as_str() {\r\n            \"docx\" => self.extract_docx(path).await,\r\n            \"pdf\" => self.extract_pdf(path).await,\r\n            \"xlsx\" => self.extract_xlsx(path).await,\r\n            \"pptx\" => self.extract_pptx(path).await,\r\n            // Plain text files can be read directly\r\n            \"txt\" | \"md\" | \"rst\" | \"log\" => {\r\n                tokio::fs::read_to_string(path).await.map_err(Into::into)\r\n            }\r\n            _ => {\r\n                // For unknown file types, try to read as text first\r\n                // If that fails, mark as binary file\r\n                match tokio::fs::read_to_string(path).await {\r\n                    Ok(content) => Ok(content),\r\n                    Err(_) => Ok(format!(\"[Binary file: {}]\", path.display())),\r\n                }\r\n            }\r\n        }\r\n    }\r\n    \r\n    /// Extracts text content from Microsoft Word DOCX files\r\n    /// DOCX files are ZIP archives containing XML files with the document content\r\n    async fn extract_docx(&self, path: &Path) -> Result<String> {\r\n        // Try to extract text from DOCX using external tools\r\n        self.try_docx_extraction(path).await\r\n    }\r\n    \r\n    /// Attempts to extract text from DOCX files using multiple methods\r\n    /// DOCX files are ZIP archives containing XML files, so we can extract them\r\n    /// \r\n    /// # Arguments\r\n    /// * `path` - Path to the DOCX file\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<String>` - Extracted text or placeholder message\r\n    async fn try_docx_extraction(&self, path: &Path) -> Result<String> {\r\n        // Method 1: Try pandoc first (if available)\r\n        // Pandoc is a universal document converter that handles DOCX well\r\n        let pandoc_output = Command::new(\"pandoc\")\r\n            .args(&[path.to_str().unwrap(), \"-t\", \"plain\"])\r\n            .output();\r\n            \r\n        if let Ok(output) = pandoc_output {\r\n            if output.status.success() {\r\n                let content = String::from_utf8_lossy(&output.stdout).to_string();\r\n                if !content.trim().is_empty() {\r\n                    return Ok(content);\r\n                }\r\n            }\r\n        }\r\n        \r\n        // Method 2: Try unzip + basic XML parsing\r\n        // DOCX files are ZIP archives containing word/document.xml with the main content\r\n        let unzip_output = Command::new(\"unzip\")\r\n            .args(&[\"-p\", path.to_str().unwrap(), \"word/document.xml\"])\r\n            .output();\r\n            \r\n        if let Ok(output) = unzip_output {\r\n            if output.status.success() {\r\n                let xml_content = String::from_utf8_lossy(&output.stdout);\r\n                // Basic XML text extraction - remove Word-specific tags\r\n                // This is a simplified approach that works for most documents\r\n                let text = xml_content\r\n                    .replace(\"<w:t>\", \"\")  // Remove text start tags\r\n                    .replace(\"</w:t>\", \" \")  // Replace text end tags with spaces\r\n                    .replace(\"<[^>]*>\", \"\")  // Remove all remaining XML tags\r\n                    .replace(\"&lt;\", \"<\")   // Decode HTML entities\r\n                    .replace(\"&gt;\", \">\")\r\n                    .replace(\"&amp;\", \"&\")\r\n                    .replace(\"&quot;\", \"\\\"\")\r\n                    .replace(\"&apos;\", \"'\");\r\n                \r\n                if !text.trim().is_empty() {\r\n                    return Ok(text);\r\n                }\r\n            }\r\n        }\r\n        \r\n        // If all methods fail, return a placeholder message\r\n        log::warn!(\"Could not extract DOCX content from: {}\", path.display());\r\n        Ok(format!(\"[DOCX file: {} - content extraction failed]\", path.display()))\r\n    }\r\n    \r\n    /// Extracts text content from PDF files\r\n    /// PDFs are complex binary formats that require specialized extraction\r\n    async fn extract_pdf(&self, path: &Path) -> Result<String> {\r\n        // Try internal PDF parsing first (currently disabled)\r\n        match self.extract_pdf_with_lopdf(path).await {\r\n            Ok(content) if !content.trim().is_empty() => Ok(content),\r\n            _ => {\r\n                // Fallback to external pdftotext tool\r\n                self.try_pdftotext(path).await\r\n            }\r\n        }\r\n    }\r\n    \r\n    /// Attempts to extract PDF content using the lopdf Rust library\r\n    /// Currently disabled due to compilation issues with the dependency\r\n    async fn extract_pdf_with_lopdf(&self, path: &Path) -> Result<String> {\r\n        // lopdf temporarily disabled due to compilation issues\r\n        log::info!(\"PDF extraction with lopdf not available for: {}\", path.display());\r\n        Ok(String::new())\r\n    }\r\n    \r\n    /// Attempts to extract PDF content using the external pdftotext tool\r\n    /// pdftotext is part of the poppler-utils package and is widely available\r\n    /// \r\n    /// # Arguments\r\n    /// * `path` - Path to the PDF file\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<String>` - Extracted text or error message\r\n    async fn try_pdftotext(&self, path: &Path) -> Result<String> {\r\n        let output = Command::new(\"pdftotext\")\r\n            .args(&[path.to_str().unwrap(), \"-\"]) // Read from file, write to stdout\r\n            .output()?;\r\n        \r\n        if output.status.success() {\r\n            Ok(String::from_utf8_lossy(&output.stdout).to_string())\r\n        } else {\r\n            log::warn!(\"pdftotext failed for {}: {}\", path.display(), String::from_utf8_lossy(&output.stderr));\r\n            Ok(format!(\"[Failed to extract PDF content: pdftotext not available]\"))\r\n        }\r\n    }\r\n    \r\n    /// Extracts text content from Microsoft Excel XLSX files\r\n    /// XLSX files are ZIP archives containing XML files with spreadsheet data\r\n    async fn extract_xlsx(&self, path: &Path) -> Result<String> {\r\n        // Try to extract text from XLSX using external tools\r\n        self.try_xlsx_extraction(path).await\r\n    }\r\n    \r\n    /// Attempts to extract text from XLSX files using multiple methods\r\n    /// XLSX files store text content in xl/sharedStrings.xml\r\n    /// \r\n    /// # Arguments\r\n    /// * `path` - Path to the XLSX file\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<String>` - Extracted text or placeholder message\r\n    async fn try_xlsx_extraction(&self, path: &Path) -> Result<String> {\r\n        // Method 1: Try pandoc first (if available)\r\n        // Pandoc can handle XLSX files and convert them to plain text\r\n        let pandoc_output = Command::new(\"pandoc\")\r\n            .args(&[path.to_str().unwrap(), \"-t\", \"plain\"])\r\n            .output();\r\n            \r\n        if let Ok(output) = pandoc_output {\r\n            if output.status.success() {\r\n                let content = String::from_utf8_lossy(&output.stdout).to_string();\r\n                if !content.trim().is_empty() {\r\n                    return Ok(content);\r\n                }\r\n            }\r\n        }\r\n        \r\n        // Method 2: Try unzip + basic XML parsing\r\n        // XLSX files store text content in xl/sharedStrings.xml\r\n        let unzip_output = Command::new(\"unzip\")\r\n            .args(&[\"-p\", path.to_str().unwrap(), \"xl/sharedStrings.xml\"])\r\n            .output();\r\n            \r\n        if let Ok(output) = unzip_output {\r\n            if output.status.success() {\r\n                let xml_content = String::from_utf8_lossy(&output.stdout);\r\n                // Basic XML text extraction - remove Excel-specific tags\r\n                let text = xml_content\r\n                    .replace(\"<t>\", \"\")  // Remove text start tags\r\n                    .replace(\"</t>\", \" \")  // Replace text end tags with spaces\r\n                    .replace(\"<[^>]*>\", \"\")  // Remove all remaining XML tags\r\n                    .replace(\"&lt;\", \"<\")   // Decode HTML entities\r\n                    .replace(\"&gt;\", \">\")\r\n                    .replace(\"&amp;\", \"&\")\r\n                    .replace(\"&quot;\", \"\\\"\")\r\n                    .replace(\"&apos;\", \"'\");\r\n                \r\n                if !text.trim().is_empty() {\r\n                    return Ok(text);\r\n                }\r\n            }\r\n        }\r\n        \r\n        // If all methods fail, return a placeholder message\r\n        log::warn!(\"Could not extract XLSX content from: {}\", path.display());\r\n        Ok(format!(\"[XLSX file: {} - content extraction failed]\", path.display()))\r\n    }\r\n    \r\n    /// Extracts text content from Microsoft PowerPoint PPTX files\r\n    /// PPTX files are ZIP archives containing XML files with slide content\r\n    async fn extract_pptx(&self, path: &Path) -> Result<String> {\r\n        // Try to extract text from PPTX using external tools\r\n        self.try_pptx_extraction(path).await\r\n    }\r\n    \r\n    /// Attempts to extract text from PPTX files using multiple methods\r\n    /// PPTX files store text content in ppt/slides/slide*.xml files\r\n    /// \r\n    /// # Arguments\r\n    /// * `path` - Path to the PPTX file\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<String>` - Extracted text or placeholder message\r\n    async fn try_pptx_extraction(&self, path: &Path) -> Result<String> {\r\n        // Method 1: Try pandoc first (if available)\r\n        // Pandoc can handle PPTX files and convert them to plain text\r\n        let pandoc_output = Command::new(\"pandoc\")\r\n            .args(&[path.to_str().unwrap(), \"-t\", \"plain\"])\r\n            .output();\r\n            \r\n        if let Ok(output) = pandoc_output {\r\n            if output.status.success() {\r\n                let content = String::from_utf8_lossy(&output.stdout).to_string();\r\n                if !content.trim().is_empty() {\r\n                    return Ok(content);\r\n                }\r\n            }\r\n        }\r\n        \r\n        // Method 2: Try unzip + basic XML parsing\r\n        // PPTX files store text content in ppt/slides/slide*.xml files\r\n        // We start with slide1.xml as a representative sample\r\n        let unzip_output = Command::new(\"unzip\")\r\n            .args(&[\"-p\", path.to_str().unwrap(), \"ppt/slides/slide1.xml\"])\r\n            .output();\r\n            \r\n        if let Ok(output) = unzip_output {\r\n            if output.status.success() {\r\n                let xml_content = String::from_utf8_lossy(&output.stdout);\r\n                // Basic XML text extraction - remove PowerPoint-specific tags\r\n                let text = xml_content\r\n                    .replace(\"<a:t>\", \"\")  // Remove text start tags\r\n                    .replace(\"</a:t>\", \" \")  // Replace text end tags with spaces\r\n                    .replace(\"<[^>]*>\", \"\")  // Remove all remaining XML tags\r\n                    .replace(\"&lt;\", \"<\")   // Decode HTML entities\r\n                    .replace(\"&gt;\", \">\")\r\n                    .replace(\"&amp;\", \"&\")\r\n                    .replace(\"&quot;\", \"\\\"\")\r\n                    .replace(\"&apos;\", \"'\");\r\n                \r\n                if !text.trim().is_empty() {\r\n                    return Ok(text);\r\n                }\r\n            }\r\n        }\r\n        \r\n        // If all methods fail, return a placeholder message\r\n        log::warn!(\"Could not extract PPTX content from: {}\", path.display());\r\n        Ok(format!(\"[PowerPoint file: {} - content extraction failed]\", path.display()))\r\n    }\r\n}\r\n",
    "file_size": 12229,
    "modified": 1756990459,
    "file_type": "rs"
  },
  "src\\search\\mod.rs": {
    "content": "pub mod engine;\r\n",
    "file_size": 17,
    "modified": 1756944722,
    "file_type": "rs"
  },
  "src\\lib.rs": {
    "content": "pub mod analysis;\r\npub mod file_discovery;\r\npub mod platform;\r\npub mod search;\r\npub mod io;\r\npub mod extraction;\r\npub mod integrations;",
    "file_size": 135,
    "modified": 1756987066,
    "file_type": "rs"
  },
  "src\\main.rs": {
    "content": "use clap::{Parser, Subcommand};  // Command-line argument parsing\r\nuse std::path::PathBuf;  // Cross-platform path handling\r\nuse anyhow::Result;  // Error handling\r\n\r\n// Module declarations for all FERRET components\r\nmod file_discovery;  // File discovery and intelligent grouping\r\nmod io;  // I/O utilities and file operations\r\nmod analysis;  // Duplicate detection and file analysis\r\nmod search;  // Search engine and content search\r\nmod extraction;  // Document content extraction\r\nmod integrations;  // External tool integrations\r\nmod platform;  // Main platform orchestration\r\n\r\nuse platform::FerretPlatform;  // Main platform for coordinating all functionality\r\n\r\n/// Command-line interface for FERRET\r\n/// This defines the CLI structure and all available commands\r\n#[derive(Parser)]\r\n#[command(name = \"ferret\")]\r\n#[command(about = \"A powerful file analysis and search tool for cleaning up messy shared drives\")]\r\nstruct Cli {\r\n    #[command(subcommand)]\r\n    command: Commands,\r\n}\r\n\r\n/// Available FERRET commands\r\n/// Each command provides a specific functionality for file analysis and search\r\n#[derive(Subcommand)]\r\nenum Commands {\r\n    /// Analyze directory for duplicates and similar files\r\n    /// This is the main analysis command that finds duplicate files and groups similar ones\r\n    Analyze {\r\n        /// Directory to analyze\r\n        path: PathBuf,\r\n        /// Output format (json, text)\r\n        #[arg(short, long, default_value = \"text\")]\r\n        format: String,\r\n    },\r\n    /// Search for files and content\r\n    /// This command searches for text within files, including document content\r\n    Search {\r\n        /// Search query\r\n        query: String,\r\n        /// Directory to search in\r\n        path: PathBuf,\r\n        /// Maximum number of results\r\n        #[arg(short, long, default_value = \"100\")]\r\n        limit: usize,\r\n    },\r\n    /// Index directory for fast searching\r\n    /// This command creates a persistent index for much faster subsequent searches\r\n    Index {\r\n        /// Directory to index\r\n        path: PathBuf,\r\n        /// Index location\r\n        #[arg(short, long, default_value = \"./ferret_index\")]\r\n        index_path: PathBuf,\r\n    },\r\n}\r\n\r\n/// Main entry point for FERRET\r\n/// This function parses command-line arguments and executes the appropriate command\r\n#[tokio::main]\r\nasync fn main() -> Result<()> {\r\n    // Initialize logging for debugging and error reporting\r\n    env_logger::init();\r\n    \r\n    // Parse command-line arguments\r\n    let cli = Cli::parse();\r\n    \r\n    // Execute the requested command\r\n    match cli.command {\r\n        Commands::Analyze { path, format } => {\r\n            // Validate that the path exists\r\n            if !path.exists() {\r\n                eprintln!(\"Error: Path does not exist: {}\", path.display());\r\n                std::process::exit(1);\r\n            }\r\n            \r\n            // Create platform and analyze the directory\r\n            let mut platform = FerretPlatform::new()?;\r\n            let results = platform.analyze_directory(&path).await?;\r\n            \r\n            // Output results in the requested format\r\n            match format.as_str() {\r\n                \"json\" => println!(\"{}\", serde_json::to_string_pretty(&results)?),\r\n                _ => println!(\"{:#?}\", results),\r\n            }\r\n        }\r\n        Commands::Search { query, path, limit } => {\r\n            // Validate search query\r\n            if query.is_empty() {\r\n                eprintln!(\"Error: Search query cannot be empty\");\r\n                std::process::exit(1);\r\n            }\r\n            \r\n            // Validate limit parameter\r\n            if limit == 0 {\r\n                eprintln!(\"Error: Limit must be greater than 0\");\r\n                std::process::exit(1);\r\n            }\r\n            \r\n            // Validate that the path exists\r\n            if !path.exists() {\r\n                eprintln!(\"Error: Path does not exist: {}\", path.display());\r\n                std::process::exit(1);\r\n            }\r\n            \r\n            // Create platform for search\r\n            let mut platform = FerretPlatform::new()?;\r\n            \r\n            // Check if there's an index in the current directory\r\n            // If an index exists, use it for much faster searching\r\n            let index_path = std::path::Path::new(\"./ferret_index\");\r\n            if index_path.exists() {\r\n                // Load the existing index for fast searching\r\n                let engine = crate::search::engine::RipgrepSearchEngine::new(index_path)?;\r\n                platform.set_search_engine(engine);\r\n            }\r\n            \r\n            // Perform the search\r\n            let results = platform.search(&query, &path, limit).await?;\r\n            \r\n            // Display search results with rich formatting\r\n            for (i, result) in results.iter().enumerate() {\r\n                if i > 0 {\r\n                    println!(); // Add spacing between results\r\n                }\r\n                \r\n                // Header with file info and metadata\r\n                println!(\"🔍 {} (Score: {:.1}, {} matches, {} bytes, .{})\", \r\n                    result.path.display(), \r\n                    result.score,\r\n                    result.match_count,\r\n                    result.file_size,\r\n                    result.file_type\r\n                );\r\n                \r\n                // Context lines before the match\r\n                for (j, context_line) in result.context_before.iter().enumerate() {\r\n                    let base_line = result.line_number.unwrap_or(0);\r\n                    let line_num = base_line.saturating_sub((result.context_before.len() - j) as u64);\r\n                    println!(\"  {:3} │ {}\", line_num, context_line);\r\n                }\r\n                \r\n                // Main match line (highlighted with arrow)\r\n                if let Some(line_num) = result.line_number {\r\n                    println!(\"  {:3} │ {} ← MATCH\", line_num, result.snippet);\r\n                }\r\n                \r\n                // Context lines after the match\r\n                for (j, context_line) in result.context_after.iter().enumerate() {\r\n                    let line_num = result.line_number.unwrap_or(0) + (j + 1) as u64;\r\n                    println!(\"  {:3} │ {}\", line_num, context_line);\r\n                }\r\n            }\r\n        }\r\n        Commands::Index { path, index_path } => {\r\n            // Validate that the path exists\r\n            if !path.exists() {\r\n                eprintln!(\"Error: Path does not exist: {}\", path.display());\r\n                std::process::exit(1);\r\n            }\r\n            \r\n            // Create platform and index the directory\r\n            let mut platform = FerretPlatform::new()?;\r\n            platform.index_directory(&path, &index_path).await?;\r\n            println!(\"Indexing complete for: {}\", path.display());\r\n        }\r\n    }\r\n    \r\n    Ok(())\r\n}\r\n",
    "file_size": 6906,
    "modified": 1756990929,
    "file_type": "rs"
  },
  "src\\integrations\\mod.rs": {
    "content": "",
    "file_size": 0,
    "modified": 1756987058,
    "file_type": "rs"
  },
  "src\\io\\mod.rs": {
    "content": "",
    "file_size": 0,
    "modified": 1756987051,
    "file_type": "rs"
  },
  "src\\analysis\\duplicates.rs": {
    "content": "use std::collections::HashMap;\r\nuse std::path::{Path, PathBuf};\r\nuse sha2::{Sha256, Digest};\r\nuse anyhow::Result;\r\nuse tokio::io::AsyncReadExt;\r\nuse crate::file_discovery::FileGroup;\r\n\r\n/// Detects exact duplicates using SHA-256 hashing\r\n/// This is a critical component for identifying files with identical content\r\n/// It processes file groups and finds files that have the same hash\r\npub struct SmartDuplicateDetector;\r\n\r\nimpl SmartDuplicateDetector {\r\n    /// Creates a new SmartDuplicateDetector instance\r\n    pub fn new() -> Self {\r\n        Self\r\n    }\r\n    \r\n    /// Detects exact duplicates within file groups\r\n    /// This is the main entry point for duplicate detection\r\n    /// \r\n    /// # Arguments\r\n    /// * `file_groups` - Groups of similar files to analyze\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<DuplicateResults>` - Results containing all found duplicates\r\n    pub async fn detect_duplicates(&self, file_groups: &[FileGroup]) -> Result<DuplicateResults> {\r\n        let mut results = DuplicateResults::new();\r\n        \r\n        // Process each group of similar files\r\n        for group in file_groups {\r\n            // Only process groups that have multiple files (potential duplicates)\r\n            if group.is_potential_duplicate() {\r\n                // Find actual duplicates within this group using SHA-256 hashing\r\n                let duplicate_sets = self.find_exact_duplicates_in_group(group).await?;\r\n                \r\n                // If duplicates were found, add them to the results\r\n                if !duplicate_sets.is_empty() {\r\n                    results.add_duplicate_group(DuplicateGroup {\r\n                        base_name: group.canonical_name.clone(),\r\n                        duplicate_sets,\r\n                    });\r\n                }\r\n            }\r\n        }\r\n        \r\n        Ok(results)\r\n    }\r\n    \r\n    /// Finds exact duplicates within a file group using SHA-256 hashing\r\n    /// This is the core algorithm for identifying files with identical content\r\n    /// \r\n    /// # Arguments\r\n    /// * `group` - File group to analyze for duplicates\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<Vec<Vec<PathBuf>>>` - List of duplicate sets (each set contains files with same content)\r\n    async fn find_exact_duplicates_in_group(&self, group: &FileGroup) -> Result<Vec<Vec<PathBuf>>> {\r\n        // Hash all files in the group and group by hash\r\n        let mut hash_map: HashMap<String, Vec<PathBuf>> = HashMap::new();\r\n        \r\n        for file_path in &group.variants {\r\n            // Skip if file doesn't exist or can't be read\r\n            if !file_path.exists() {\r\n                log::warn!(\"File doesn't exist: {:?}\", file_path);\r\n                continue;\r\n            }\r\n            \r\n            // Calculate SHA-256 hash of the file content\r\n            match self.hash_file(file_path).await {\r\n                Ok(hash) => {\r\n                    // Group files by their hash - files with same hash are duplicates\r\n                    hash_map.entry(hash)\r\n                        .or_insert_with(Vec::new)\r\n                        .push(file_path.clone());\r\n                }\r\n                Err(e) => {\r\n                    // Log hash failures but continue processing other files\r\n                    log::warn!(\"Failed to hash file {:?}: {}\", file_path, e);\r\n                }\r\n            }\r\n        }\r\n        \r\n        // Return only groups with 2+ files (actual duplicates)\r\n        // Files with the same hash are considered exact duplicates\r\n        Ok(hash_map\r\n            .into_values()\r\n            .filter(|group| group.len() > 1)\r\n            .collect())\r\n    }\r\n    \r\n    /// Calculates SHA-256 hash of a file for duplicate detection\r\n    /// This is the core method for identifying files with identical content\r\n    /// \r\n    /// # Arguments\r\n    /// * `path` - Path to the file to hash\r\n    /// \r\n    /// # Returns\r\n    /// * `Result<String>` - Hexadecimal representation of the SHA-256 hash\r\n    async fn hash_file(&self, path: &Path) -> Result<String> {\r\n        let mut file = tokio::fs::File::open(path).await?;\r\n        let mut hasher = Sha256::new();\r\n        let mut buffer = vec![0u8; 8192]; // 8KB buffer for efficient reading\r\n        \r\n        // Read file in chunks to handle large files efficiently\r\n        loop {\r\n            let bytes_read = file.read(&mut buffer).await?;\r\n            if bytes_read == 0 {\r\n                break; // End of file reached\r\n            }\r\n            // Update hash with the chunk we just read\r\n            hasher.update(&buffer[..bytes_read]);\r\n        }\r\n        \r\n        // Return hexadecimal representation of the hash\r\n        Ok(format!(\"{:x}\", hasher.finalize()))\r\n    }\r\n}\r\n\r\n/// Results of duplicate detection analysis\r\n/// Contains statistics and all found duplicate groups\r\n#[derive(Debug, serde::Serialize)]\r\npub struct DuplicateResults {\r\n    /// Total number of duplicate files found (excluding originals)\r\n    pub total_duplicates: usize,\r\n    /// Total space wasted by duplicate files in bytes\r\n    pub space_wasted: u64,\r\n    /// All groups of duplicate files found\r\n    pub duplicate_groups: Vec<DuplicateGroup>,\r\n}\r\n\r\nimpl DuplicateResults {\r\n    /// Creates a new empty DuplicateResults instance\r\n    pub fn new() -> Self {\r\n        Self {\r\n            total_duplicates: 0,\r\n            space_wasted: 0,\r\n            duplicate_groups: Vec::new(),\r\n        }\r\n    }\r\n    \r\n    /// Adds a duplicate group and updates statistics\r\n    /// This method calculates the total duplicates and wasted space\r\n    /// \r\n    /// # Arguments\r\n    /// * `group` - Duplicate group to add\r\n    pub fn add_duplicate_group(&mut self, group: DuplicateGroup) {\r\n        // Calculate statistics for this group\r\n        for duplicate_set in &group.duplicate_sets {\r\n            if duplicate_set.len() > 1 {\r\n                // Count actual duplicate files (all but one original)\r\n                self.total_duplicates += duplicate_set.len() - 1;\r\n                \r\n                // Calculate wasted space (size of all duplicates except the first)\r\n                for file in duplicate_set.iter().skip(1) {\r\n                    if let Ok(metadata) = std::fs::metadata(file) {\r\n                        self.space_wasted += metadata.len();\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        \r\n        // Add the group to our results\r\n        self.duplicate_groups.push(group);\r\n    }\r\n}\r\n\r\n/// Represents a group of duplicate files with the same base name\r\n/// Each group can contain multiple sets of identical files\r\n#[derive(Debug, serde::Serialize)]\r\npub struct DuplicateGroup {\r\n    /// The base name used for grouping these files\r\n    pub base_name: String,\r\n    /// Each inner vec contains files that are identical to each other\r\n    pub duplicate_sets: Vec<Vec<PathBuf>>,\r\n}",
    "file_size": 6837,
    "modified": 1756990661,
    "file_type": "rs"
  }
}